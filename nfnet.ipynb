{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom tensorflow.keras.datasets import cifar10 as cifar\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom numpy import random as npr\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport numpy as np\nimport time, sys, gc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mixed Precision Setup","metadata":{}},{"cell_type":"code","source":"useMixed = True\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    useMixed = False\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(gpus[0], True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if useMixed:\n    print('---MIXED PRECISION TRAINING---')\n    mixed_precision.set_global_policy('mixed_bfloat16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameters","metadata":{}},{"cell_type":"code","source":"m = 50000\nif useMixed:\n    batchSize = 1024\nelse:\n    batchSize = 256\n    \nepochs = 300\nsteps_per_epoch = 1000\np = 0.0\npStep = 1e-2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load data and augment function","metadata":{}},{"cell_type":"code","source":"(train_x, train_y), (test_x, test_y) = cifar.load_data()\n\ndef normalize(batch):\n    mean = np.mean(batch, axis=(0, 1, 2), keepdims=True)\n    std = np.std(batch, axis=(0, 1, 2), keepdims=True)\n    return (batch - mean) / std\n\ntrain_x = normalize(train_x[:m])\ntest_x = normalize(test_x)\ntrain_y = tf.keras.utils.to_categorical(train_y)[:m]\ntest_y = tf.keras.utils.to_categorical(test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aug(imgs, p):\n    imgSize = 32\n    augImgs = tf.cast(imgs, tf.float32)\n    def augCond(x):\n        randInds = tf.random.uniform((batchSize, 1, 1, 1))\n        trueCond = tf.cast(randInds < p, tf.float32) # using tf.cast to turn booleans into ones and zeros\n        falseCond = 1 - trueCond\n        auged = x * trueCond + augImgs * falseCond\n        return auged\n    \n    height = tf.random.uniform((), minval=0.5, maxval=1)\n    width = tf.random.uniform((), minval=0.5, maxval=1)\n    boxLite = tf.random.uniform((batchSize, 2), maxval=(1-height, 1-width))\n    boxes = tf.concat([boxLite, tf.transpose(boxLite[:, 0][np.newaxis]) + height, tf.transpose(boxLite[:, 1][np.newaxis]) + width], axis=1)\n    boxLiteIso = tf.random.uniform((batchSize, 1), maxval=1-height)\n    boxIso = tf.concat([boxLite, tf.transpose(boxLiteIso[:, 0][np.newaxis]) + height, tf.transpose(boxLiteIso[:, 0][np.newaxis]) + height], axis=1)\n    rot90s = np.pi * 90 * tf.cast(tf.random.uniform((batchSize,), minval=0, maxval=4, dtype=tf.int32), tf.float32) / 180\n    augImgs = augCond(tf.image.random_brightness(augImgs, max_delta=0.25))\n    augImgs = augCond(tf.image.crop_and_resize(augImgs, boxIso, tf.range(batchSize), (imgSize, imgSize), extrapolation_value=1))\n    augImgs = augCond(tf.image.crop_and_resize(augImgs, boxes, tf.range(batchSize), (imgSize, imgSize), extrapolation_value=1))\n    augImgs = augCond(tfa.image.rotate(augImgs, rot90s, fill_mode='reflect'))\n    augImgs = augCond(tfa.image.rotate(augImgs, tf.random.uniform((batchSize,), minval=-np.pi/6, maxval=np.pi/6), fill_mode='reflect'))\n    augImgs = augCond(tfa.image.translate(augImgs, tf.random.normal((batchSize, 2), 0, imgSize // 10), fill_mode='reflect'))\n    return augImgs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom Layers","metadata":{}},{"cell_type":"code","source":"ndist = tf.random_normal_initializer(0, 1)\nlecun = tf.keras.initializers.LecunNormal()\nzeros = tf.zeros_initializer()\nones = tf.ones_initializer()\nl2 = tf.keras.regularizers.L2(l2=2e-5)\n\ncastZero = tf.convert_to_tensor(0.0)\nmagic = tf.convert_to_tensor((2 / (1 - (1 / np.pi))) ** 0.5)\nif useMixed:\n    castZero = tf.cast(castZero, tf.bfloat16)\n    magic = tf.cast(magic, tf.bfloat16)\n\nactivation = Lambda(lambda x: tf.maximum(castZero, x) * magic)\n \nclass WSConv(Conv2D):\n    def __init__(self, units, kernel_size=3, kernel_initializer=ndist, bias_initializer=zeros, kernel_regularizer=l2, padding='same', relu=True, *args, **kwargs):\n        super().__init__(units, kernel_size, *args, **kwargs)\n        self.kernel_initializer = kernel_initializer\n        self.bias_initializer = bias_initializer\n        self.kernel_regularizer = kernel_regularizer\n        self.padding = padding\n        if kernel_size != 1:\n            self.groups = units // (scale // 2)\n        self.relu = relu\n        self.scale = 1\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        self.fan_in = np.prod(self.kernel.shape[:-1]) # self.kernel.shape = (kernel_x, kernel_y, features_in, features_out)\n        self.gain = self.add_weight(name='gain', shape=(self.kernel.shape[-1],), initializer='ones', trainable=True)\n\n    def call(self, inputs):\n        mean = tf.math.reduce_mean(self.kernel, axis=(0, 1, 2), keepdims=True)\n        var = tf.math.reduce_variance(self.kernel, axis=(0, 1, 2), keepdims=True)\n        k = self.gain * (self.kernel - mean) * tf.math.rsqrt(tf.maximum(var * self.fan_in, 1e-8))\n        output = K.conv2d(inputs, k, padding=self.padding, strides=self.strides)\n        output = K.bias_add(output, self.bias)\n        \n        if self.relu:\n            output = activation(output)\n        return output\n    \nclass SE(Layer):\n    def __init__(self, nf, se_ratio=0.5, activation=activation, name=None):\n        super(SE, self).__init__(name=name)\n        self.activation = activation\n        self.fc0 = Dense(int(se_ratio * nf), kernel_initializer=lecun, kernel_regularizer=l2, use_bias=True)\n        self.fc1 = Dense(nf, kernel_initializer=lecun, kernel_regularizer=l2, use_bias=True)\n\n    def call(self, x):\n        h = tf.math.reduce_mean(x, axis = [1, 2])\n        h = self.fc1(activation(self.fc0(h)))\n        h = tf.keras.activations.sigmoid(h)[:, None, None]\n        return 2 * x * h\n\nclass StochDepth(Layer):\n    def __init__(self, dropRate=0.25):\n        super(StochDepth, self).__init__()\n        self.dropRate = dropRate\n    \n    def call(self, x, training):\n        batchSize = tf.shape(x)[0]\n        \n        if not training:\n            return x, tf.ones([batchSize, 1, 1, 1], dtype=x.dtype)\n        \n        randNums = tf.random.uniform(shape=[batchSize, 1, 1, 1], dtype=x.dtype)\n        keepRate = 1 - self.dropRate\n        keepInds = tf.floor(keepRate + randNums)\n        return x * keepInds, keepInds\n\nclass NFBlock(Layer):\n    def __init__(self, units, stochDepth, strides=1, skipconv=False, alpha=0.2):\n        super(NFBlock, self).__init__()\n        self.alpha = alpha\n        self.strides = strides\n        self.skipconv = skipconv\n        if self.skipconv:\n            self.skipConv = WSConv(units, 1, relu=False)\n            \n        self.conv1 = WSConv(units // 2, 1)\n        self.conv2 = WSConv(units // 2, strides=strides)\n        self.conv3 = WSConv(units // 2)\n        self.conv4 = WSConv(units, 1, relu=False)\n        self.se = SE(units)\n        self.stochDepth = StochDepth(stochDepth)\n        \n    def build(self, input_shape):\n        super().build(input_shape)\n        self.skipInitGain = self.add_weight(name='skip_init_gain', shape=(), trainable=True, initializer='zeros')\n    \n    def call(self, inputs):\n        out, var = inputs\n        beta = var ** 0.5\n        \n        if self.skipconv:\n            out = out * (1 / beta) # var = 1\n            out = activation(out)\n\n        skip = out\n        if self.strides == 2:\n            skip = AveragePooling2D()(skip)\n        if self.skipconv:\n            skip = self.skipConv(skip)\n        \n        if self.strides == 1:\n            out = out * (1 / beta)\n            out = activation(out)\n            \n        out = self.conv1(out)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        out = self.conv4(out)\n        \n        out = self.se(out) * self.alpha * self.skipInitGain\n        out, stochSkips = self.stochDepth(out)\n        out = Add()([skip, out])\n        \n        if self.skipconv:\n            var = 1 + stochSkips * (alpha * self.skipInitGain) ** 2\n        else:\n            var = var + stochSkips * (alpha * self.skipInitGain) ** 2\n        return out, var","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building model","metadata":{}},{"cell_type":"code","source":"def build(units=(1, 2, 6, 6), repeats=(1, 2, 6, 3), scale=64, repeatScale=1):\n    inp = Input((32, 32, 3))\n    out = inp\n\n    calcStochDepth = lambda ind: 0.25 * ind / sum(units)\n    blockIdx = 0\n    var = 1.0\n    alpha = 0.2\n    for idx, unit in enumerate(units):\n        if idx == 0: # put var in call arg to prevent sicko mode\n            out, var = NFBlock(unit * scale, calcStochDepth(blockIdx), skipconv=True, strides=1, alpha=alpha)([out, var])\n        else:\n            out, var = NFBlock(unit * scale, calcStochDepth(blockIdx), skipconv=True, strides=2, alpha=alpha)([out, var])\n        blockIdx += 1\n\n        for _ in range(repeatScale * repeats[idx] - 1):\n            out, var = NFBlock(unit * scale, calcStochDepth(blockIdx), alpha=alpha)([out, var])\n            blockIdx += 1\n        \n    out = WSConv(2 * units[-1] * scale, 1)(out)\n    out = tf.reduce_mean(out, axis=[1, 2])\n    out = Flatten()(out)\n    out = Dropout(0.2)(out)\n    out = Dense(100, kernel_initializer=tf.random_normal_initializer(0, 0.01), kernel_regularizer=l2)(out) # 0.01 thing sus\n    out = Activation('softmax', dtype=tf.float32)(out)\n    return Model(inp, out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step function","metadata":{}},{"cell_type":"code","source":"loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1, reduction=tf.keras.losses.Reduction.SUM)\nrecipBatch = tf.convert_to_tensor(1 / batchSize)\n\n@tf.function(experimental_compile=useMixed)\ndef trainStep(x, y):\n    def norm(tensor, axes):\n        return tf.sqrt(tf.reduce_sum(tf.square(tensor), axis=axes))\n    \n    def clip(grad, w):\n        lb = 1e-2\n        eps = 1e-3\n        fanInRange = tf.range(tf.maximum(0, tf.rank(grad)-1))\n        gw = norm(grad, axes=fanInRange) / tf.maximum(norm(w, axes=fanInRange), eps)\n        smolG = tf.cast(gw < lb, grad.dtype)\n        bigG  = 1 - smolG\n        clipGrad = smolG * grad + lb / tf.maximum(gw, 1e-6) * bigG * grad\n        return clipGrad\n\n    preds = model(x, training=True)\n\n    loss = loss_fn(y, preds) * recipBatch\n\n    grad = tf.gradients(loss, model.trainable_variables)\n    clipGrad = [clip(grad[wi], w) for wi, w in enumerate(model.trainable_variables[:-2])] + grad[-2:] # :-2 excludes clipping on last linear layer with bias\n    opt.apply_gradients(zip(clipGrad, model.trainable_variables))\n    return loss\n\n@tf.function(experimental_compile=useMixed)\ndef evalBatch(x, y):\n    return loss_fn(y, model(x, training=False)) * recipBatch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train function","metadata":{}},{"cell_type":"code","source":"def toNp(mean):\n    ret = []\n    \n    if useMixed:\n        mean = tf.reduce_mean(mean.values)\n    \n    mean = mean.numpy()\n    return mean\n\ndef train(batchSize, epochs=1, steps=5, evaluate=True):\n    global p, pStep\n    \n    for i in range(epochs):\n        stepNum = 0\n        cost = 0\n        \n        batchRandInds = npr.randint(0, m, (steps, batchSize))\n        validRandInds = npr.randint(0, test_x.shape[0], (steps // 16 + 1, batchSize))\n        for step in tqdm(range(steps)):\n            randInds = batchRandInds[step]\n            batchX = aug(train_x[randInds], p)\n            batchY = train_y[randInds]\n            \n            if useMixed:\n                loss = toNp(tpu_strategy.run(trainStep, args=(batchX, batchY)))\n            else:\n                loss = toNp(trainStep(batchX, batchY))\n            \n            if step % 16 == 0:\n                randInds = validRandInds[step // 16]\n                validX = test_x[randInds]\n                validY = test_y[randInds]\n                if useMixed:\n                    valid_loss = tf.reduce_mean(tpu_strategy.run(\n                        evalBatch, args=(validX, validY)).values).numpy()\n                else:\n                    valid_loss = evalBatch(validX, validY).numpy()\n                    \n                p += pStep * np.sign(valid_loss / loss - 1.1) # if val loss high, train loss low, up augment\n                p = max(p, 0) % 1.0\n            \n            cost += loss\n        print('Epoch {} | Train Loss: {}'.format(i, cost / steps))\n        if evaluate:\n            model.evaluate(test_x, test_y, batch_size=batchSize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom learning rate scheduler","metadata":{}},{"cell_type":"code","source":"class HybridLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, lr=(1e-1 * batchSize / 256), decay_steps=((epochs - 5) * steps_per_epoch), warmup=(5 * steps_per_epoch)):\n        super().__init__()\n        self.maxlr = lr\n        self.decSteps = decay_steps\n        self.warmSteps = warmup\n    \n    def __call__(self, step):\n        def cosDecay(step):\n            step = tf.minimum(step, tf.cast(self.decSteps, step.dtype))\n            cosine_decay = 0.5 * (1 + tf.math.cos(np.pi * step / self.decSteps))\n            return self.maxlr * cosine_decay\n        \n        lr = tf.cond(step < tf.cast(self.warmSteps, step.dtype),\n                     lambda: step / self.warmSteps * self.maxlr,\n                     lambda: cosDecay(step - tf.cast(self.warmSteps, step.dtype)))\n        return lr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the model and compiling","metadata":{}},{"cell_type":"code","source":"if useMixed:\n    with tpu_strategy.scope():\n        scale = 256\n        repeatScale = 1\n        model = build(scale=scale, repeatScale=repeatScale)\n        print('Scale: {} | Num params: {}'.format(scale, model.count_params()))\n        opt = tf.keras.optimizers.SGD(learning_rate=HybridLR(), momentum=0.9, nesterov=True)\n        model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])\nelse:\n    scale = 256\n    repeatScale = 1\n    model = build(scale=scale, repeatScale=repeatScale)\n    print('Scale: {} | Num params: {}'.format(scale, model.count_params()))\n    opt = tf.keras.optimizers.SGD(learning_rate=HybridLR(), momentum=0.9, nesterov=True)\n    model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train model","metadata":{}},{"cell_type":"code","source":"train(batchSize, epochs, steps=steps_per_epoch)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batchX = train_x[batchSize:2*batchSize]\nfor i in range(3):\n    plt.imshow(batchX[i])\n    plt.show()\nprint(model.predict(batchX))\nprint(loss_fn(train_y[batchSize:2*batchSize], model.predict(batchX)) / batchSize)\nprint(p, pStep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate model","metadata":{}},{"cell_type":"code","source":"model.evaluate(train_x, train_y, batch_size=batchSize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}